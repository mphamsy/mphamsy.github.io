<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>T5 Question Generation from SQuAD | Michal Pham Sy</title> <meta name="author" content="Michal Pham Sy"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mphamsy.github.io/projects/qg_squad/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Michal </span>Pham Sy</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Project Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">T5 Question Generation from SQuAD</h1> <p class="post-description"></p> </header> <article> <div class="row"> <div class="col-md-6 offset-md-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/qgmain.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/qgmain.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/qgmain.JPG-1400.webp"></source> <img src="/assets/img/qgmain.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Question Generation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="problem-definition"><strong>Problem Definition</strong></h3> <p>Written and online assessments are the core of examination processes for most educational institutions. The students are required to undertake exams every year with an ever-changing curriculum, which puts a significant strain on tutors and teachers who are responsible for producing and generating the assessments on the annual basis. To tackle this issue, the concept of intelligent tutoring systems (ITS) is being explored.</p> <p>Intelligent tutoring systems can create a substantial quantity of educational resources through the implementation of a large-scale and multi-domain knowledge base.The technological advance and availability of digital data sources resulted in the exploration of <strong>automatic question generation (AQG)</strong> as the potential alternative to manual question generation (QG) for ITS.</p> <h3 id="answer-aware-and-answer-agnostic-models"><strong>Answer-aware and answer-agnostic models</strong></h3> <p>Deep neural networks are used to generate questions from a given passage in a process known as neural question generation (NQG). The NQG is split into two categories of <strong>answer-aware</strong> and <strong>answer-unaware</strong> model types. In answer-aware models, both context and corresponding answer are supplied to generate a question, whereas answer-unaware models are only given the context.</p> <p>As of now, <strong>state-of-the-art (SOTA) systems are answer-aware</strong> NQG as these vastly outperform answer-unaware and non-NQG approaches. Answer-aware models tend to generate better contextually and syntactically sound questions and therefore will be more suitable for QG tasks in the context of intelligent tutoring systems.</p> <h3 id="requirements"><strong>Requirements</strong></h3> <p>This project is written in Python and requires the following libraries to be <strong>installed</strong> and <strong>imported</strong>.</p> <script src="https://gist.github.com/mphamsy/c1b0a6abaa68ac2ccfe1ac13b10c7d22.js"></script> <h3 id="dataset"><strong>Dataset</strong></h3> <p>The source of question-generation data is the one provided by <strong>Stanford Question Answering Dataset (SQuAD)</strong> version 1. <strong>SQuAD_v1</strong> is a large-scale reading comprehension dataset consisting of approximately 98 thousand text paragraphs from Wikipedia, each supplemented with a complementary ground truth question and corresponding answer. Additionally, the dataset highlights individually answer locations within each paragraph. Figure below represents a random sample from the dataset.</p> <script src="https://gist.github.com/mphamsy/7bb405b5b921933b76c5043e8d541a3a.js"></script> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Length of the training dataset:  87599
Length of the validation dataset:  10570
</code></pre></div></div> <p>The dataset overall contains parapgraph id, context, question, answer and answer starts. The DataFrame table was parsed, and paragraph ids was removed. The visualisation of the data is provided below.</p> <div class="row"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/qgdata.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/qgdata.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/qgdata.JPG-1400.webp"></source> <img src="/assets/img/qgdata.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Squad dataset" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of the content inside the SQuAD Dataset </div> <p><br></p> <h3 id="preprocessing"><strong>Preprocessing</strong></h3> <p>All QG pre-processing strategies are answer-aware systems, where both answer and context are used together as a text string input to train the model. The formats of train and test datasets were revised according to each of the pre-processing strategies and were later used in the post-processing for corresponding methods to generate questions.</p> <p>The pre-processing strategy input consists of an answer followed by a context. The answer is separated from the context by a designated <sep> token responsible for separating two various sentences in the same text string input.</sep></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Prepending Encoder Input = Answer + &lt;SEP&gt; + Context
</code></pre></div></div> <div class="row"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/qgprep.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/qgprep.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/qgprep.JPG-1400.webp"></source> <img src="/assets/img/qgprep.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Prepending Formatting" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The prepending formatting </div> <script src="https://gist.github.com/mphamsy/836bbaa7e57f7b2d962662d5a9ddf41c.js"></script> <h3 id="transformers"><strong>Transformers</strong></h3> <p>The Text-to-Text Transfer Transformer (T5) is a completely text to text transformer proposed by Raffel et al. 2020. The T5 unifies NLP frameworks including question generation, summarization, sentiment analysis, language modelling, question answering or span extraction. Most importantly, T5 as a text-to-text frame can apply the same model, goal, training steps and decoding to every required task.</p> <p>The T5 was trained on the Colossal Clean Crawled Corpus (C4), a dataset consisting of 750GB of cleaned and parsed English text taken from the web. Due to a sheer number of parameters taken into consideration while training, the T5 model has 5 size types, each with varying architecture to accommodate for parameter processing. T5-base type will be used to generate questions.</p> <div class="row"> <div class="col"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/qgarch.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/qgarch.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/qgarch.JPG-1400.webp"></source> <img src="/assets/img/qgarch.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="T5 Architecture" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> T5 Architecture </div> <script src="https://gist.github.com/mphamsy/21a38d49c08c6821c63ed1ff2fee10d3.js"></script> <h3 id="feature-conversion"><strong>Feature Conversion</strong></h3> <p>The data was subsequently converted to features, collated and fed to the transformer T5.</p> <script src="https://gist.github.com/mphamsy/3430b3e551fd75e1fac1304bf2c1c58f.js"></script> <script src="https://gist.github.com/mphamsy/dab0427464b17f196f9b2312cf227b96.js"></script> <h3 id="training"><strong>Training</strong></h3> <p>Following the feature conversion, the model can be trained. The dataset was sliced and only 26000 training and 5000 test samples were used during the training of the transformer for 7 epochs. All hyperparameters used can be seen in the following code. Note that the model is pushed onto HuggingFace, which requires an account to generate a write token to push the model to the hub. Otherwise, the model can be saved locally.</p> <script src="https://gist.github.com/mphamsy/2434244d1229f2a0e07ade46e5aa8eff.js"></script> <h3 id="testing"><strong>Testing</strong></h3> <p>The model was subsequently trained and pushed to the hub. The pre-trained t5 was then tested on context paragraphs and answer pairs to generate the questions.</p> <script src="https://gist.github.com/mphamsy/fc5514c1a6f095f02460a3158220158c.js"></script> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Answer + Context: four &lt;sep&gt; On January 7, 2012, Beyonce gave birth to her first child, a daughter, Blue Ivy Carter, at Lenox Hill Hospital in New York. Five months later, she performed for four nights at Revel Atlantic City's Ovation Hall to celebrate the resort's opening, her first performances since giving birth to Blue Ivy.
Actual: How many nights did Beyonce play at the resort?
Predicted: How many nights did Beyonce perform at the Ovation Hall?

Answer + Context: Bach and Mozart &lt;sep&gt; Chopin was educated in the tradition of Beethoven, Haydn, Mozart and Clementi; he used Clementi's piano method with his own students. He was also influenced by Hummel's development of virtuoso, yet Mozartian, piano technique. He cited Bach and Mozart as the two most important composers in shaping his musical outlook. Chopin's early works are in the style of the "brilliant" keyboard pieces of his era as exemplified by the works of Ignaz Moscheles, Friedrich Kalkbrenner, and others. Less direct in the earlier period are the influences of Polish folk music and of Italian opera.
Actual: Who did Chopin say were the two most important composers in his own music influences?
Predicted: Which two composers did Chopin consider the most important in his musical outlook?
    
Answer + Context: Zhu Yuanzhang &lt;sep&gt; In 1368, a Han Chinese revolt known as the Red Turban Rebellion toppled the Mongol Yuan dynasty in China. Zhu Yuanzhang then established the Ming dynasty, ruling as the Hongwu Emperor (r. 1368-1398). It is not clear how much the early Ming court understood the civil war going on in Tibet between rival religious sects, but the first emperor was anxious to avoid the same trouble that Tibet had caused for the Tang dynasty. 
Actual: Who created the Ming Dynasty? 
Predicted: Who established the Ming dynasty?
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Michal Pham Sy. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>